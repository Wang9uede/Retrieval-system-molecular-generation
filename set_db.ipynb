{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:05:47.030214Z",
     "start_time": "2024-06-27T03:05:42.625406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from molt5_dataset import Mol2CaptionDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from molxpt_tokenizer import  MolxptTokenizer\n",
    "# 假设 raw_folder 和 pro_folder 是你的数据文件夹路径，mode 是你的数据模式\n",
    "import weaviate\n",
    "import re\n",
    "raw_folder = '/Users/wangjuede/Documents/mol2cap_data'\n",
    "mode = 'train'  # 可以是 'train', 'val' 或 'test'\n",
    "\n",
    "# 实例化数据集\n"
   ],
   "id": "ac7ab9a37c539fa5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:05:48.007541Z",
     "start_time": "2024-06-27T03:05:47.673486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = MolxptTokenizer.from_pretrained(\"molxpt_ckpt\", use_fast=False)\n",
    "\n",
    "dataset = Mol2CaptionDataset(raw_folder, mode, tokenizer=tokenizer)"
   ],
   "id": "55f73e41914b06a0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BioGptTokenizer'. \n",
      "The class this function is called from is 'MolxptTokenizer'.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T16:10:19.535401Z",
     "start_time": "2024-06-02T16:10:19.483636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    if i < len(dataset):\n",
    "        item = dataset[i]  # 获取第 i 个数据项\n",
    "        print(item[1])  # 打印编码后的 caption\n",
    "        "
   ],
   "id": "d57bcf71726b9814",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,    28,  1494,    30,    47, 21079,    18, 11514,    17, 20806,\n",
      "         14861, 15297,   808,    32,    30,    10,  6209,  1679,     9,   514,\n",
      "          3899,    93,    18,  1905,   404,    11,  1801,   474,    17,   106,\n",
      "         13820, 19811,    18,   311,  1683,    11,   435,  1683,    11,  1530,\n",
      "           463,    17,   106, 20806, 14861, 17871,   244,    11,   273,    31,\n",
      "           866, 14384,     9,    10, 36023,    85,    61,   356,   299,    48,\n",
      "           596,  5870,     8,   242,    30,    22,  6209,  1679,     9,    47,\n",
      "           514,  3899,    93,    18,  1905,   404,    11,  1801,   474,    17,\n",
      "           106, 13820, 19811,    18,   311,  1683,    11,   435,  1683,    11,\n",
      "          1530,   463,    17,   106, 20806, 14861, 17871,   244,     8]])\n",
      "tensor([[    2,    28,  1494,    30,    10,   953,  5857,     9,   653,  3487,\n",
      "         22584,    23,   652,  4159,   457,  1530,  1277,  4252,  8362,   494,\n",
      "            11, 25363,  4311,  1462,   971,  3307,    13,   918,  4616,    71,\n",
      "            37,    65,     8]])\n",
      "tensor([[    2,    28,  1494,    30,    22, 15867, 18302, 27888,  2057,    32,\n",
      "            30,  4750,  4750,   303,     3,   106,  6399, 12967, 35802,    22,\n",
      "             3,   106,  6399,  2655,   497,     3,   106,  6399,   607,  2681,\n",
      "          3742,    31,    47,  1652,   603,    85,    48,  1129,    90,    11,\n",
      "            47,  4717, 11081,    85,    48,  1129,   291,    13,    22, 26679,\n",
      "             3,   106,  6399,    90,     3,   106,  6399,  1452,    85,    48,\n",
      "          1129,   796,     8,  9266,    41, 11413,  6242,   675,  4080,  4621,\n",
      "         24589,    11,   136,  3834,   849,  9138, 11057,   830,    13, 17312,\n",
      "           595,     8,   242,    81,    22,   173,    40,    47, 17312,  1051,\n",
      "            11,    47,   849,  9138, 11057,   830,   264,    13,    22,  1013,\n",
      "          2891,     8,   242,    30,    47,  3866,  3355,    11,    22,  2622,\n",
      "          2047, 34429, 15687,    11,    22,  3639,     9,  5825,  1842,    11,\n",
      "            22,  6723,   831,  2057,    13,    22, 15867, 18302, 27888,  2057,\n",
      "             8]])\n",
      "tensor([[    2,    28,  1494,    30,    22,  3639,     9,    10,  1120,     9,\n",
      "           370,     3,   106,  6399, 12750,  3818, 38315,    32,    30,  5316,\n",
      "            15,    64,   133,     9,    10,  5180, 19139,    30,  3742,    31,\n",
      "          2797,    13,  5180,  1215,   157,     8,   242,    81,    22,   173,\n",
      "            40,    22,  8708,  1051,    11,    22, 21159,  1851,    11,    22,\n",
      "         22640,  1051,    13,    47, 16136,  1051,     8]])\n",
      "tensor([[    2,    28,  1494,    30,    47, 15496,  8491,   835, 20899,    32,\n",
      "            30, 15496,  3742,    48,  1129,    90,    31,    22, 32620,     3,\n",
      "           106,  6399,  3018, 20704,     3,   106,  6399,    65,     3,   106,\n",
      "          6399,  1452,    85,     8,   242,    81,    22,   173,    40,    22,\n",
      "          2891,     8,   242,    30,    47, 15496,  8491,   835, 20899,    13,\n",
      "            22,  3639,     9, 32620,     3,   106,  6399, 20426,   662,     8]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:06:28.947392Z",
     "start_time": "2024-06-27T03:06:21.580432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"molxpt_ckpt\")\n",
    "# 访问嵌入层\n",
    "embedding_layer = model.biogpt.embed_tokens"
   ],
   "id": "b419169094397289",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T20:37:03.561049Z",
     "start_time": "2024-06-17T20:37:03.515651Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset[1])",
   "id": "339087fb58831ba9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[125Te]', tensor([[    2,    28,  1494,    30,    10,   953,  5857,     9,   653,  3487,\n",
      "         22584,    23,   652,  4159,   457,  1530,  1277,  4252,  8362,   494,\n",
      "            11, 25363,  4311,  1462,   971,  3307,    13,   918,  4616,    71,\n",
      "            37,    65,     8]]), 'The molecule is the stable isotope of tellurium with relative atomic mass 124.904425, 71.4 atom percent natural abundance and nuclear spin 1/2.')\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T16:31:12.479630Z",
     "start_time": "2024-06-02T16:31:04.908387Z"
    }
   },
   "cell_type": "code",
   "source": "print(model(torch.tensor(dataset[1][1])).hidden_states[-2])",
   "id": "1c147a153ded5804",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kf/myby0xcs14j14p4sg5dgxnbc0000gn/T/ipykernel_62884/1424585921.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(model(torch.tensor(dataset[1][1])).hidden_states[-2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T11:56:32.628218Z",
     "start_time": "2024-06-26T11:56:28.578931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BioGptModel, BioGptTokenizer\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model = BioGptModel.from_pretrained('molxpt_ckpt')\n"
   ],
   "id": "b7878ffce11a4bc5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:16:13.035881Z",
     "start_time": "2024-06-27T06:16:11.786951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#连接数据库\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\n",
    "  url=\"https://rag-zm3s034w.weaviate.network\",\n",
    "  auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"VwYY6pZ6pJYvCgj4E05yLzEvbdhFuOJ4L0IA\"),\n",
    ")\n",
    "client.is_ready()"
   ],
   "id": "13d4236a98c12486",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T05:37:58.424790Z",
     "start_time": "2024-06-18T05:37:58.015132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_obj = {\n",
    "    \"class\": \"Compound_caption\",\n",
    "    \"vectorizer\": \"none\",\n",
    "}\n",
    "\n",
    "# Add the class to the schema\n",
    "client.schema.create_class(class_obj)\n",
    "\n"
   ],
   "id": "94137436baa48539",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(model(torch.tensor(dataset[1][1])))",
   "id": "8604f7ac1f289a9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T20:39:15.186770Z",
     "start_time": "2024-06-17T20:37:53.897058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "updated_dataset = []\n",
    "with torch.no_grad():\n",
    "# 遍历原始数据集中的每个元组\n",
    "    for item in dataset:\n",
    "        # 对于每个元组，创建一个新的元组，其中第二个元素是使用嵌入层生成的向量\n",
    "        # 假设 item[1] 是一个可以被嵌入层处理的列表或张量\n",
    "        embedded_item = embedding_layer(item[1])\n",
    "        pooling_item = torch.mean(embedded_item,dim=1)\n",
    "        \n",
    "        # 将原始的第一个元素和新生成的嵌入向量作为第二个元素组合成新的元组\n",
    "        new_item = (item[0], pooling_item, item[2])\n",
    "        \n",
    "        # 将新的元组添加到更新后的数据集列表中\n",
    "        updated_dataset.append(new_item)\n",
    "print(updated_dataset[1])"
   ],
   "id": "a84680e5d9a54676",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[125Te]', tensor([[-0.0068,  0.0009, -0.0018,  ...,  0.0103, -0.0030,  0.0130]]), 'The molecule is the stable isotope of tellurium with relative atomic mass 124.904425, 71.4 atom percent natural abundance and nuclear spin 1/2.')\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:44:12.354443Z",
     "start_time": "2024-06-02T17:44:12.339531Z"
    }
   },
   "cell_type": "code",
   "source": "print(updated_dataset[2][1].size())",
   "id": "7bb389dd948eefd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Configure a batch process\n",
    "client.batch.configure(batch_size=5)  # Configure batch\n",
    "for i, (label, tensor,caption) in enumerate(updated_dataset):\n",
    "    if i >= 11535:\n",
    "        try:\n",
    "            # 尝试从 Weaviate 获取对象\n",
    "            existing_object = client.get('Compound_caption', label, ['Smiles', 'caption'])\n",
    "            print(f\"Object {i+1} already exists. Skipping...\")\n",
    "        except Exception as e:\n",
    "            # 如果在获取时发生异常（例如，对象不存在），则处理异常并继续导入\n",
    "            print(f\"Importing: {i+1}\")\n",
    "            vector = tensor.view(-1).tolist()  # Flatten the tensor and convert to list\n",
    "    \n",
    "            # 创建 Weaviate 需要的对象属性字典\n",
    "            properties = {\n",
    "                \"Smiles\": label,\n",
    "                \"caption\": caption,\n",
    "                # 其他属性...\n",
    "            }\n",
    "    \n",
    "            # 使用批量客户端导入新对象\n",
    "            with client.batch as batch:\n",
    "                # 添加数据对象到批量请求中\n",
    "                batch.add_data_object(properties, \"Compound_caption\", vector=vector)\n",
    "\n",
    "# 提交批量导入\n",
    "client.batch.flush()\n"
   ],
   "id": "dfbc939668072d4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inference 从此开始",
   "id": "d9328bf2267ec35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:16:24.397261Z",
     "start_time": "2024-06-27T06:16:24.315678Z"
    }
   },
   "cell_type": "code",
   "source": "molxpt_tokenizer = MolxptTokenizer.from_pretrained(\"molxpt_ckpt\", use_fast=False)\n",
   "id": "38e40b5141660043",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BioGptTokenizer'. \n",
      "The class this function is called from is 'MolxptTokenizer'.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:20:51.796842Z",
     "start_time": "2024-06-27T06:20:51.772321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retro(vector):\n",
    "    client = weaviate.Client(\n",
    "      url=\"https://rag-zm3s034w.weaviate.network\",\n",
    "      auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"VwYY6pZ6pJYvCgj4E05yLzEvbdhFuOJ4L0IA\"),\n",
    "    )\n",
    "    client.is_ready()\n",
    "    \n",
    "    retrieval = client.query.get(\n",
    "    \"Compound_caption\",[\"smiles\",\"caption\"]).with_near_vector(\n",
    "    vector\n",
    "    ).with_limit(2).do()\n",
    "    return retrieval\n",
    "    \n",
    "def generate_mol(input_text, prompt_template, tokenizer=molxpt_tokenizer):\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    input_embedded = torch.mean(embedding_layer(input_ids),dim=1)\n",
    "        \n",
    "    retro_vec = {\n",
    "        \"vector\": input_embedded.view(-1).tolist(),\n",
    "    }\n",
    "    \n",
    "    retrieval = retro(retro_vec)\n",
    "    \n",
    "\n",
    "    inputs = prompt_template.format(\n",
    "                                    caption_1 = retrieval[\"data\"][\"Get\"][\"Compound_caption\"][0][\"caption\"],\n",
    "                                    caption_2 =retrieval[\"data\"][\"Get\"][\"Compound_caption\"][1][\"caption\"],\n",
    "                                    smiles_1 = retrieval[\"data\"][\"Get\"][\"Compound_caption\"][0][\"smiles\"],\n",
    "                                    smiles_2 = retrieval[\"data\"][\"Get\"][\"Compound_caption\"][1][\"smiles\"],\n",
    "                                    caption = input_text,\n",
    "                                    )\n",
    "    \n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.75,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    s = tokenizer.decode(output[0])\n",
    "    start_index = s.find(\"SMILES of this molecule is\") + len(\"SMILES of this molecule is\")#定位\n",
    "    #正则 \n",
    "    substring = s[start_index:]\n",
    "    match = re.search(r'<start-of-mol>(.*?)<end-of-mol>', substring)\n",
    "    if match:\n",
    "        smiles = match.group(1)  # 这是匹配到的SMILES字符串\n",
    "    else:\n",
    "        smiles = \"No SMILES found\"\n",
    "    return smiles,inputs\n",
    "\n",
    "def generate_mol_molxpt(input_text, prompt_template, tokenizer=molxpt_tokenizer):\n",
    "    inputs = prompt_template.format(caption = input_text,\n",
    "                                    )\n",
    "    #print(inputs)\n",
    "    input_ids = molxpt_tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.75,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    #print(output)\n",
    "    s = molxpt_tokenizer.decode(output[0])\n",
    "    \n",
    "    all_matches = re.findall(r'<start-of-mol>(.*?)<end-of-mol>', s)\n",
    "\n",
    "    # 检查是否有足够的匹配项\n",
    "    if len(all_matches) > 1:\n",
    "        # 获取第二个匹配项\n",
    "        smiles = all_matches[1]\n",
    "        #print(2)\n",
    "        \n",
    "    elif len(all_matches) == 1:\n",
    "        smiles = all_matches[0]\n",
    "        #print(1)\n",
    "    else:\n",
    "        smiles = \"No SMILES found\"\n",
    "    return smiles\n",
    "    "
   ],
   "id": "a3ec16357c2e3b71",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:20:58.544235Z",
     "start_time": "2024-06-27T06:20:54.176676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Here are two template molecule. First. {caption_1} SMILES of this molecule is <start-of-mol>{smiles_1}<end-of-mol>. Second. {caption_2} SMILES of this molecule is <start-of-mol>{smiles_2}<end-of-mol>. Now. {caption} SMILES of this molecule is \"\n",
    "\n",
    "prompt_molxpt = \"{caption}.The compound is <start-of-mol>\"\n",
    "\n",
    "input_text = \"The molecule is a bile acid taurine conjugate of ursocholic acid. It has a role as a human metabolite and a rat metabolite. It derives from an ursocholic acid. It is a conjugate acid of a tauroursocholate.\"\n",
    "print(generate_mol(input_text,prompt))\n"
   ],
   "id": "be363dc3dd6f232a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C[C@H](CCC(=O)NCCS(=O)(=O)O)[C@H]1CC[C@@H]2[C@@]1(CC[C@H]3[C@H]2CC[C@H]4[C@@]3(CC[C@H](C4)O)C)C', 'Here are two template molecule. First. The molecule is the bile acid taurine conjugate of lithocholic acid. It has a role as a human metabolite. It is a monocarboxylic acid amide and a bile acid taurine conjugate. It derives from a lithocholic acid. It is a conjugate acid of a taurolithocholate. SMILES of this molecule is <start-of-mol>C[C@H](CCC(=O)NCCS(=O)(=O)O)[C@H]1CC[C@@H]2[C@@]1(CC[C@H]3[C@H]2CC[C@H]4[C@@]3(CC[C@H](C4)O)C)C<end-of-mol>. Second. The molecule is a bile acid taurine conjugate of cholic acid that usually occurs as the sodium salt of bile in mammals. It has a role as a human metabolite. It is an amino sulfonic acid and a bile acid taurine conjugate. It derives from a cholic acid. It is a conjugate acid of a taurocholate. SMILES of this molecule is <start-of-mol>C[C@H](CCC(=O)NCCS(=O)(=O)O)[C@H]1CC[C@@H]2[C@@]1([C@H](C[C@H]3[C@H]2[C@@H](C[C@H]4[C@@]3(CC[C@H](C4)O)C)O)O)C<end-of-mol>. Now. The molecule is a bile acid taurine conjugate of ursocholic acid. It has a role as a human metabolite and a rat metabolite. It derives from an ursocholic acid. It is a conjugate acid of a tauroursocholate. SMILES of this molecule is ')\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T11:56:56.028284Z",
     "start_time": "2024-06-26T11:56:54.666675Z"
    }
   },
   "cell_type": "code",
   "source": "generate_mol_molxpt(input_text, prompt_molxpt)",
   "id": "f53af5e9ec737435",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ocsbcnconosococc.Isosnbonbo.Isonsococc.Isconcoosoco..copons'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Evaluation",
   "id": "f699705bfd30efe7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T11:55:37.106776Z",
     "start_time": "2024-06-26T11:55:37.091889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "mode = \"test\"\n",
    "test_data = Mol2CaptionDataset(raw_folder, mode, tokenizer=molxpt_tokenizer)\n",
    "json_file_path = '/Users/wangjuede/Downloads/molxpt/molxpt_code/similarity_results.json'\n",
    " \n",
    "# 读取JSON文件并将其转换为字典\n",
    "with open(json_file_path, 'r') as file:\n",
    "    results = json.load(file)"
   ],
   "id": "9dc7ec66a3ab7973",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:18:49.286449Z",
     "start_time": "2024-06-26T03:06:29.467733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rdkit import DataStructs,Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolDescriptors,Fingerprints\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def Fingerprint_sim(mode,std_mol,gen_mol):\n",
    "    if mode == \"maccs\":\n",
    "        std_fp = rdMolDescriptors.GetMACCSKeysFingerprint(std_mol)\n",
    "        gen_fp = rdMolDescriptors.GetMACCSKeysFingerprint(gen_mol)\n",
    "    elif mode == \"rdk\":\n",
    "        std_fp = Chem.RDKFingerprint(std_mol)\n",
    "        gen_fp = Chem.RDKFingerprint(gen_mol)\n",
    "    elif mode == \"morgan\":\n",
    "        std_fp = AllChem.GetMorganFingerprintAsBitVect(std_mol, radius=2)\n",
    "        gen_fp = AllChem.GetMorganFingerprintAsBitVect(gen_mol, radius=2)\n",
    "    tanimoto_sim = DataStructs.TanimotoSimilarity(std_fp, gen_fp)\n",
    "    return tanimoto_sim\n",
    "    \n",
    "\n",
    "i=0\n",
    "for smiles,_,caption in test_data:\n",
    "    if results.get(smiles) is not None:\n",
    "        \n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            smiles_gen = generate_mol(caption,prompt)\n",
    "            \n",
    "            std_mol = Chem.MolFromSmiles(smiles)\n",
    "            gen_mol = Chem.MolFromSmiles(smiles_gen)\n",
    "            #有效性检验\n",
    "            if not std_mol or not gen_mol:\n",
    "                results[smiles] = {\"error\": \"Invalid molecule\"}\n",
    "                continue\n",
    "            tanimoto_sim_maccs = Fingerprint_sim(\"maccs\",std_mol,gen_mol)\n",
    "            tanimoto_sim_rdk = Fingerprint_sim(\"rdk\",std_mol,gen_mol)\n",
    "            tanimoto_sim_morgan = Fingerprint_sim(\"morgan\",std_mol,gen_mol)\n",
    "          \n",
    "            results[smiles] = { \"gen_mol\" : gen_mol,\n",
    "                                \"tanimoto_sim_maccs\": tanimoto_sim_maccs,\n",
    "                                \"tanimoto_sim_rdk\": tanimoto_sim_rdk,\n",
    "                                \"tanimoto_sim_morgan\": tanimoto_sim_morgan\n",
    "            }\n",
    "        except:\n",
    "            continue\n",
    "    i += 1\n",
    "    print(i)\n",
    "results_json = json.dumps(results, indent=4)\n",
    "\n",
    "# 将JSON字符串存储到文件\n",
    "with open('similarity_results.json', 'w') as json_file:\n",
    "    json_file.write(results_json)\n",
    "\n",
    "print('相似度结果已成功存储为JSON格式。')"
   ],
   "id": "c8881c2fc6cd07c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:54:22] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "相似度结果已成功存储为JSON格式。\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from rdkit import DataStructs,Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from rdkit.Chem import rdMolDescriptors,Fingerprints\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def Fingerprint_sim(mode,std_mol,gen_mol):\n",
    "    if mode == \"maccs\":\n",
    "        std_fp = rdMolDescriptors.GetMACCSKeysFingerprint(std_mol)\n",
    "        gen_fp = rdMolDescriptors.GetMACCSKeysFingerprint(gen_mol)\n",
    "    elif mode == \"rdk\":\n",
    "        std_fp = Chem.RDKFingerprint(std_mol)\n",
    "        gen_fp = Chem.RDKFingerprint(gen_mol)\n",
    "    elif mode == \"morgan\":\n",
    "        std_fp = AllChem.GetMorganFingerprintAsBitVect(std_mol, radius=2)\n",
    "        gen_fp = AllChem.GetMorganFingerprintAsBitVect(gen_mol, radius=2)\n",
    "    tanimoto_sim = DataStructs.TanimotoSimilarity(std_fp, gen_fp)\n",
    "    return tanimoto_sim\n",
    "    \n",
    "results = {}\n",
    "i=0\n",
    "for smiles,_,caption in test_data:\n",
    "    if results.get(smiles) is not None:\n",
    "        i+=1\n",
    "        print(\"exist,{}\".format(i))\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            smiles_gen = generate_mol_molxpt(caption,prompt_molxpt)\n",
    "            \n",
    "            std_mol = Chem.MolFromSmiles(smiles)\n",
    "            gen_mol = Chem.MolFromSmiles(smiles_gen)\n",
    "            #有效性检验\n",
    "            if not std_mol or not gen_mol:\n",
    "                results[smiles] = {\"error\": \"Invalid molecule\"}\n",
    "                continue\n",
    "            tanimoto_sim_maccs = Fingerprint_sim(\"maccs\",std_mol,gen_mol)\n",
    "            tanimoto_sim_rdk = Fingerprint_sim(\"rdk\",std_mol,gen_mol)\n",
    "            tanimoto_sim_morgan = Fingerprint_sim(\"morgan\",std_mol,gen_mol)\n",
    "          \n",
    "            results[smiles] = { \"gen_mol\" : smiles_gen,\n",
    "                                \"tanimoto_sim_maccs\": tanimoto_sim_maccs,\n",
    "                                \"tanimoto_sim_rdk\": tanimoto_sim_rdk,\n",
    "                                \"tanimoto_sim_morgan\": tanimoto_sim_morgan\n",
    "            }\n",
    "            i+=1\n",
    "            print(i)\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            i+=1\n",
    "            print(i)\n",
    "            continue\n",
    "        \n",
    "results_json = json.dumps(results, indent=4)\n",
    "\n",
    "# 将JSON字符串存储到文件\n",
    "with open('similarity_results_molxpt.json', 'w') as json_file:\n",
    "    json_file.write(results_json)\n",
    "\n",
    "print('相似度结果已成功存储为JSON格式。')"
   ],
   "id": "da3746fa7d6e2a3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T13:59:35.242058Z",
     "start_time": "2024-06-26T13:59:35.152051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_json = json.dumps(results, indent=4)\n",
    "print(1)\n",
    "# 将JSON字符串存储到文件\n",
    "with open('similarity_results_molxpt.json', 'w') as json_file:\n",
    "    json_file.write(results_json)\n",
    "\n",
    "print('相似度结果已成功存储为JSON格式。')"
   ],
   "id": "94a1337635498555",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Mol is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m results_json \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# 将JSON字符串存储到文件\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/__init__.py:234\u001B[0m, in \u001B[0;36mdumps\u001B[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONEncoder\n\u001B[0;32m--> 234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskipkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskipkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_ascii\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_ascii\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_circular\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_circular\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseparators\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseparators\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py:201\u001B[0m, in \u001B[0;36mJSONEncoder.encode\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    199\u001B[0m chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterencode(o, _one_shot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 201\u001B[0m     chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(chunks)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py:431\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[0;34m(o, _current_indent_level)\u001B[0m\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_list(o, _current_indent_level)\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(o, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 431\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _iterencode_dict(o, _current_indent_level)\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    433\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py:405\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[0;34m(dct, _current_indent_level)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    404\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 405\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py:405\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode_dict\u001B[0;34m(dct, _current_indent_level)\u001B[0m\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    404\u001B[0m             chunks \u001B[38;5;241m=\u001B[39m _iterencode(value, _current_indent_level)\n\u001B[0;32m--> 405\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m chunks\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newline_indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    407\u001B[0m     _current_indent_level \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py:438\u001B[0m, in \u001B[0;36m_make_iterencode.<locals>._iterencode\u001B[0;34m(o, _current_indent_level)\u001B[0m\n\u001B[1;32m    436\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCircular reference detected\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    437\u001B[0m     markers[markerid] \u001B[38;5;241m=\u001B[39m o\n\u001B[0;32m--> 438\u001B[0m o \u001B[38;5;241m=\u001B[39m \u001B[43m_default\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m _iterencode(o, _current_indent_level)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m markers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py:179\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[0;34m(self, o)\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    177\u001B[0m \n\u001B[1;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    180\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Object of type Mol is not JSON serializable"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:10:53.284518Z",
     "start_time": "2024-06-26T16:10:53.228983Z"
    }
   },
   "cell_type": "code",
   "source": "print(results[])",
   "id": "577dc2264eadf45d",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mresults\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[0;31mKeyError\u001B[0m: 1"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:23:15.095796Z",
     "start_time": "2024-06-26T16:23:15.062367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('similarity_results_molxpt.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "sum_similarity_maccs = 0\n",
    "sum_similarity_rdk = 0\n",
    "sum_similarity_morgan = 0\n",
    "totally_similar = []\n",
    "i = 0\n",
    "# 遍历化合物数据\n",
    "for compound, similarities in data.items():\n",
    "    # 计算平均相似度\n",
    "    if \"tanimoto_sim_maccs\" in similarities:\n",
    "        i+=1\n",
    "        sum_similarity_maccs += similarities[\"tanimoto_sim_maccs\"]\n",
    "        sum_similarity_rdk += similarities[\"tanimoto_sim_rdk\"]\n",
    "        sum_similarity_morgan += similarities[\"tanimoto_sim_morgan\"]\n",
    "        if similarities[\"tanimoto_sim_maccs\"] == 1.0 and similarities[\"tanimoto_sim_rdk\"] and similarities[\"tanimoto_sim_morgan\"]:\n",
    "            totally_similar.append(compound)\n",
    "        #print(1)\n",
    "                        \n",
    "average_similarity_maccs = sum_similarity_maccs / i\n",
    "average_similarity_rdk = sum_similarity_rdk / i\n",
    "average_similarity_morgan = sum_similarity_morgan / i\n",
    "average_sim = {\n",
    "                \"tanimoto_sim_maccs\": average_similarity_maccs, \n",
    "                \"tanimoto_sim_rdk\": average_similarity_rdk, \n",
    "                \"tanimoto_sim_morgan\": average_similarity_morgan\n",
    "}\n",
    "# 输出结果\n",
    "print(average_sim)\n",
    "print(\"Totally similar molecule:\",len(totally_similar))\n",
    "print(\"Valid molecule:\",i)"
   ],
   "id": "718ac8573700350",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tanimoto_sim_maccs': 0.1813161385586092, 'tanimoto_sim_rdk': 0.0794704567512316, 'tanimoto_sim_morgan': 0.05672881185065994}\n",
      "Totally similar molecule: 0\n",
      "Valid molecule: 1778\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:24:20.596909Z",
     "start_time": "2024-06-26T16:24:20.574848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('similarity_results.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "sum_similarity_maccs = 0\n",
    "sum_similarity_rdk = 0\n",
    "sum_similarity_morgan = 0\n",
    "totally_similar = []\n",
    "i = 0\n",
    "# 遍历化合物数据\n",
    "for compound, similarities in data.items():\n",
    "    # 计算平均相似度\n",
    "    if \"tanimoto_sim_maccs\" in similarities:\n",
    "        i+=1\n",
    "        sum_similarity_maccs += similarities[\"tanimoto_sim_maccs\"]\n",
    "        sum_similarity_rdk += similarities[\"tanimoto_sim_rdk\"]\n",
    "        sum_similarity_morgan += similarities[\"tanimoto_sim_morgan\"]\n",
    "        if similarities[\"tanimoto_sim_maccs\"] == 1.0 and similarities[\"tanimoto_sim_rdk\"] and similarities[\"tanimoto_sim_morgan\"]:\n",
    "            totally_similar.append(compound)\n",
    "        #print(1)\n",
    "                        \n",
    "average_similarity_maccs = sum_similarity_maccs / i\n",
    "average_similarity_rdk = sum_similarity_rdk / i\n",
    "average_similarity_morgan = sum_similarity_morgan / i\n",
    "average_sim = {\n",
    "                \"tanimoto_sim_maccs\": average_similarity_maccs, \n",
    "                \"tanimoto_sim_rdk\": average_similarity_rdk, \n",
    "                \"tanimoto_sim_morgan\": average_similarity_morgan\n",
    "}\n",
    "# 输出结果\n",
    "print(average_sim)\n",
    "print(\"Totally similar molecule:\",len(totally_similar))\n",
    "print(\"Valid molecule:\",i)"
   ],
   "id": "9e9cc1384e95d1cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tanimoto_sim_maccs': 0.7899117205304005, 'tanimoto_sim_rdk': 0.6937007764215024, 'tanimoto_sim_morgan': 0.5697404357824769}\n",
      "Totally similar molecule: 328\n",
      "Valid molecule: 1473\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:17:18.035043Z",
     "start_time": "2024-06-27T06:17:18.010016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"The molecule is a bile acid taurine conjugate of ursocholic acid. It has a role as a human metabolite and a rat metabolite. It derives from an ursocholic acid. It is a conjugate acid of a tauroursocholate.\"\n",
    "input_ids = molxpt_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "input_embedded = torch.mean(embedding_layer(input_ids),dim=1)\n",
    "    \n",
    "retro_vec = {\n",
    "    \"vector\": input_embedded.view(-1).tolist(),\n",
    "}\n",
    "\n",
    "#print(len(retro_vec[\"vector\"]))\n",
    "#print(input_text)\n"
   ],
   "id": "e9eec2cf1886b1d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:17:21.193786Z",
     "start_time": "2024-06-27T06:17:20.945438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(retrieval[\"data\"][\"Get\"][\"Compound_caption\"][0][\"smiles\"])\n",
    "print(retrieval[\"data\"][\"Get\"][\"Compound_caption\"][0][\"caption\"])\n"
   ],
   "id": "7877e0c628c25a74",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mretrieval\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGet\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompound_caption\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msmiles\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(retrieval[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGet\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompound_caption\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'retrieval' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T11:26:00.100509Z",
     "start_time": "2024-06-26T11:25:59.083496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"{caption}.The compound is <start-of-mol>\"\n",
    "inputs = prompt.format(caption = input_text,\n",
    "                                )\n",
    "print(inputs)\n",
    "print()\n",
    "input_ids = molxpt_tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.75,\n",
    "    do_sample=True,\n",
    ")\n",
    "#print(output)\n",
    "s = molxpt_tokenizer.decode(output[0])\n",
    "print(s)\n"
   ],
   "id": "c691f54f5dd46e06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The molecule is a bile acid taurine conjugate of ursocholic acid. It has a role as a human metabolite and a rat metabolite. It derives from an ursocholic acid. It is a conjugate acid of a tauroursocholate..The compound is <start-of-mol>\n",
      "</s> <start-of-mol>ocsbcnconosococc.Isosnbonbo.Isonsococc.Isconcoosoco..copons<end-of-mol>. </s>\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "generate_mol(input_text,prompt_template=prompt)",
   "id": "9fcb1c5da4e4beef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T03:59:22.387580Z",
     "start_time": "2024-06-16T03:59:12.810681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = \"Here are two template molecule. First. {caption_1} SMILES of this molecule is <start-of-mol>{smiles_1}<end-of-mol>. Second. {caption_2} SMILES of this molecule is <start-of-mol>{smiles_2}<end-of-mol>. Now. {caption} SMILES of this molecule is \"\n",
    "\n",
    "\n",
    "inputs = prompt_template.format(caption_1 = retrieval[\"data\"][\"Get\"][\"Compound_caption\"][0][\"caption\"],\n",
    "                                caption_2 = retrieval[\"data\"][\"Get\"][\"Compound_caption\"][1][\"caption\"],\n",
    "                                smiles_1 = retrieval[\"data\"][\"Get\"][\"Compound_caption\"][0][\"smiles\"],\n",
    "                                smiles_2 = retrieval[\"data\"][\"Get\"][\"Compound_caption\"][1][\"smiles\"],\n",
    "                                caption = input_text,\n",
    "                                )\n",
    "\n",
    "input_ids = molxpt_tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.75,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(output)\n",
    "s = molxpt_tokenizer.decode(output[0])\n",
    "#s = s[len(inputs):].strip()\n",
    "print(s)"
   ],
   "id": "938fc457d7a8fe1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,   559,    46,   100,  6247,  1494,     8,  3893,     8,    28,\n",
      "          1494,    30,    22,  2708,     3,   106,  6399,  4094, 20103,  4414,\n",
      "           273,    31,   866, 14384,     9,    10, 18870,  2791,   157,     9,\n",
      "           611,   268,  5668,     3,   106,  6399,   141,     3,   106,  6399,\n",
      "         23091,   603,     3,   106,  6399,  3452,   182,     3,   106,  6399,\n",
      "           607,   866, 11081,     3,   106,  6399,   574,     3,   106,  6399,\n",
      "           427,     3,   106,  6399,   965,    61,   356,   299,    48,   596,\n",
      "          5870,     8,   242,    81,    22,   173,    40,    22,   910,  2891,\n",
      "             8,   242, 32895,    41,    22,   611,   268,  5668,     3,   106,\n",
      "          6399,   574,     3,   106,  6399,   427,     3,   106,  6399,   965,\n",
      "            18,  3340,    17,     8,   242,    30,    22,  6209,  1679,     9,\n",
      "            22,   611,   268,  5668,     3,   106,  6399,   141,     3,   106,\n",
      "          6399, 23091,   603,     3,   106,  6399,  3452,   182,     3,   106,\n",
      "          6399,   607,   866, 11081,     3,   106,  6399,   574,     3,   106,\n",
      "          6399,   427,     3,   106,  6399,   965,     8,  2911,  3362,  2971,\n",
      "             9,    52,  1494,    30, 44533,     4,    36,    14,     4,     6,\n",
      "            16,    12,     7,    36,     6,     4,    33,     6,    12,    14,\n",
      "             7,    12,   172,     6,    16,    12,     7,     6,    76,     7,\n",
      "            12,   172,     6,    16,    12,     7,     6,    76,     7,    12,\n",
      "             4,    36,    19,    33,     6,     4,    36,     6,    12,    19,\n",
      "             7,    20,    27,     4,    16,     4,     6,     4,     6,    16,\n",
      "            12,     7,    20,     4,    27,    16,    12,     7,     4,     7,\n",
      "            12,     7,    12, 44534,     8,  7218,     8,    28,  1494,    30,\n",
      "            22,  2708,     3,   106,  6399,  4094, 20103,  4414,  5821,    41,\n",
      "           866, 14384,     9,    10,   523, 18870,  2791,   157,     9,   611,\n",
      "           268,  5668,     3,   106,  6399,    90,     3,   106,  6399, 23091,\n",
      "           603,     3,   106,  6399,  4750,   182,     3,   106,  6399,   607,\n",
      "           866, 11081,     3,   106,  6399,   574,     3,   106,  6399,   427,\n",
      "             3,   106,  6399,   965,    61,   356,   299,    48,   596,  5870,\n",
      "             8,   242,    30,    22,  6209,  1679,     9,    22,   611,   268,\n",
      "          5668,     3,   106,  6399,    90,     3,   106,  6399, 23091,   603,\n",
      "             3,   106,  6399,  4750,   182,     3,   106,  6399,   607,   866,\n",
      "         11081,     3,   106,  6399,   574,     3,   106,  6399,   427,     3,\n",
      "           106,  6399,   965,     8,  2911,  3362,  2971,     9,    52,  1494,\n",
      "            30, 44533,     4,    36,    14,     4,     4,     6,    16,    12,\n",
      "             7,    33,     6,    33,     6,    12,    14,     7,    12,   172,\n",
      "             6,    16,    12,     7,     6,    76,     7,    12,   172,     6,\n",
      "            16,    12,     7,     6,    76,     7,    12,     4,    36,    19,\n",
      "            33,     6,     4,    36,     6,    12,    19,     7,    20,    27,\n",
      "             4,    16,     4,     6,     4,     6,    16,    12,     7,    20,\n",
      "             4,    27,    16,    12,     7,     4,     7,    12,     7,    12,\n",
      "         44534,     8, 26003,     8,    28,  1494,    30,    22,  2708,     3,\n",
      "           106,  6399,  4094,  1652,  5400,  1137,  5821,    41,   866, 14384,\n",
      "             9,    90,    10, 18870,  2791,   157,     9,   611,   268,  5668,\n",
      "             3,   106,  6399,   141,     3,   106,  6399, 23091,   603,     3,\n",
      "           106,  6399, 19761,   182,     3,   106,  6399,  1836,   866, 11081,\n",
      "             3,   106,  6399,   574,     3,   106,  6399,   427,     3,   106,\n",
      "          6399,   965,    34,   356,   299,    48,   596,  5870,     8,   242,\n",
      "            81,    22,   173,    40,    22,   910,  2891,     8,   242,    30,\n",
      "            22,  6209,  1679,     9,    22,   611,   268,  5668,     3,   106,\n",
      "          6399,   141,     3,   106,  6399, 23091,   603,     3,   106,  6399,\n",
      "         19761,   182,     3,   106,  6399,  1836,   866, 11081,     3,   106,\n",
      "          6399,   574,     3,   106,  6399,   427,     3,   106,  6399,   965,\n",
      "             8,  2911,  3362,  2971,     9,    52,  1494,    30, 44533,     4,\n",
      "            36,    14,     4,     4,     6,    16,    12,     7,    33,     6,\n",
      "            33,     6,    12,     5,    19,     5,     5,     5,     6,   150,\n",
      "             6,    16,    12,     7,    76,     7,     5,     5,    19,     7,\n",
      "             4,     6,    16,    12,     7,    12,     7,    33,    14,     5,\n",
      "            14,     5,     5,     5,     5,     5,    14, 44534]])\n",
      "</s>Here are two template molecule. First. The molecule is a nucleotide <unk>-@ sugar oxoanion obtained by deprotonation of the diphosphate OH groups of dTDP <unk>-@ 4 <unk>-@ dehydro <unk>-@ 2,6 <unk>-@ dideoxy <unk>-@ alpha <unk>-@ D <unk>-@ glucose; major species at pH 7.3. It has a role as a bacterial metabolite. It derives from a dTDP <unk>-@ alpha <unk>-@ D <unk>-@ glucose (2-). It is a conjugate base of a dTDP <unk>-@ 4 <unk>-@ dehydro <unk>-@ 2,6 <unk>-@ dideoxy <unk>-@ alpha <unk>-@ D <unk>-@ glucose. SMILES of this molecule is <start-of-mol>C[C@@H]1C(=O)[C@@H](C[C@H](O1)OP(=O)([O-])OP(=O)([O-])OC[C@@H]2[C@H](C[C@@H](O2)N3C=C(C(=O)NC3=O)C)O)O<end-of-mol>. Second. The molecule is a nucleotide <unk>-@ sugar oxoanion arising from deprotonation of the free diphosphate OH groups of dTDP <unk>-@ 3 <unk>-@ dehydro <unk>-@ 4,6 <unk>-@ dideoxy <unk>-@ alpha <unk>-@ D <unk>-@ glucose; major species at pH 7.3. It is a conjugate base of a dTDP <unk>-@ 3 <unk>-@ dehydro <unk>-@ 4,6 <unk>-@ dideoxy <unk>-@ alpha <unk>-@ D <unk>-@ glucose. SMILES of this molecule is <start-of-mol>C[C@@H]1CC(=O)[C@H]([C@H](O1)OP(=O)([O-])OP(=O)([O-])OC[C@@H]2[C@H](C[C@@H](O2)N3C=C(C(=O)NC3=O)C)O)O<end-of-mol>. Now. The molecule is a nucleotide <unk>-@ sugar oxcanion arising from deprotonation of 3 the diphosphate OH groups of dTDP <unk>-@ 4 <unk>-@ dehydro <unk>-@ 2,3,6 <unk>-@ trideoxy <unk>-@ alpha <unk>-@ D <unk>-@ glucose: major species at pH 7.3. It has a role as a bacterial metabolite. It is a conjugate base of a dTDP <unk>-@ 4 <unk>-@ dehydro <unk>-@ 2,3,6 <unk>-@ trideoxy <unk>-@ alpha <unk>-@ D <unk>-@ glucose. SMILES of this molecule is <start-of-mol>C[C@@H]1CC(=O)[C@H]([C@H](Oc2ccc([N+](=O)[O-])cc2)C(=O)O)[C@H]1c1ccccc1<end-of-mol>\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "406567c7c03f9f8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
